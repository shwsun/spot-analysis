{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# Spot resources Analytics\n",
    "\n",
    "Here we perform some initial process and analysis on the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With static dataset, e.g. load the grabbed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# parse the data file and extra the results\n",
    "filename = 'data1'\n",
    "\n",
    "df = pd.read_csv(filename, sep=\"\\t\", header = None)\n",
    "df.columns = [\"info\", \"SpotPrice\", \"TimeStamp\", \"InstanceType\", \"OS type\", \"AvailabilityZone\"]\n",
    "df['TimeStamp'] =pd.to_datetime(df.TimeStamp)\n",
    "\n",
    "df.index = df.TimeStamp\n",
    "df = df.drop('info', 1).drop(['OS type'],axis=1)\n",
    "df = df.drop(['TimeStamp'],axis=1).sort_index()\n",
    " \n",
    "\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (df['InstanceType'].unique())\n",
    "print (df['AvailabilityZone'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "instance_types  = ['c3.xlarge', 'c3.2xlarge', 'c3.4xlarge', 'c3.8xlarge']\n",
    "region = 'us-east-1'\n",
    "\n",
    "df1 = df[df.AvailabilityZone == 'us-west-1a']\n",
    "df2 = df1[df1.InstanceType == 'c3.8xlarge']\n",
    "df2.to_csv('us-east-1a_c3-8xlarge.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k, g in df1.sort_index(ascending=True).groupby(['InstanceType'], as_index=False):\n",
    "    for key, grp in g.groupby(['AvailabilityZone'], as_index=False):\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.plot(grp.index, grp['SpotPrice'], label=key)\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.title('Spot Pricing - ' + k)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k, g in df1.sort_index(ascending=True).groupby(['InstanceType'], as_index=False):\n",
    "    #plt.figure(1, figsize(20,5))\n",
    "    for key, grp in g.groupby(['AvailabilityZone'], as_index=False):\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.hist(grp['SpotPrice'], bins=100, label=key,)\n",
    "        plt.xlim([0, 1])\n",
    "        #grp.groupby(grp.index.dayofweek).agg(['mean']).plot()\n",
    "    plt.legend()\n",
    "    plt.title('Histogram of Spot Pricing - ' + k)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we grad dataset from AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "instance_types  = ['c3.xlarge', 'c3.2xlarge', 'c3.4xlarge', 'c3.8xlarge']\n",
    "region = 'us-east-1'\n",
    "number_of_days = 10\n",
    "\n",
    "end = !date -u \"+%Y-%m-%dT%H:%M:%S\"\n",
    "end = end[0]\n",
    "start = !date -v-{number_of_days}d -u \"+%Y-%m-%dT%H:%M:%S\"\n",
    "\n",
    "#start = !date -v-{number_of_days}d\" -u \"+%Y-%m-%dT%H:%M:%S\"\n",
    "#print(start)\n",
    "start = start[0]\n",
    "print (\"will process from \" + start + \" to \" + end)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import boto as boto\n",
    "import boto.ec2 as ec2\n",
    "import datetime, time\n",
    "#import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.mpl_style', 'default')  # Make the graphs a bit prettier\n",
    "%pylab inline\n",
    "%matplotlib inline\n",
    "\n",
    "ec2 = boto.ec2.connect_to_region(region)\n",
    "\n",
    "\n",
    "#\n",
    "# process the output and convert to a dataframe\n",
    "#\n",
    "\n",
    "l = []\n",
    "for instance in instance_types:\n",
    "    sys.stdout.write(\"*** processing \" + instance + \" ***\\n\")\n",
    "    sys.stdout.flush()\n",
    "    prices = ec2.get_spot_price_history(start_time=start, end_time=end, instance_type=instance)\n",
    "    for price in prices:\n",
    "        d = {'InstanceType': price.instance_type, \n",
    "             'AvailabilityZone': price.availability_zone, \n",
    "             'SpotPrice': price.price, \n",
    "             'Timestamp': price.timestamp}\n",
    "        l.append(d)\n",
    "    next = prices.next_token\n",
    "    while (next != ''):\n",
    "        sys.stdout.write(\".\")\n",
    "        sys.stdout.flush()\n",
    "        prices = ec2.get_spot_price_history(start_time=start, end_time=end, instance_type=instance,\n",
    "                                            next_token=next )\n",
    "        for price in prices:\n",
    "            d = {'InstanceType': price.instance_type, \n",
    "                 'AvailabilityZone': price.availability_zone, \n",
    "                 'SpotPrice': price.price, \n",
    "                 'Timestamp': price.timestamp}\n",
    "            l.append(d)\n",
    "        next = prices.next_token\n",
    "        \n",
    "    sys.stdout.write(\"\\n\")\n",
    "\n",
    "df = pd.DataFrame(l)\n",
    "df = df.set_index(pd.to_datetime(df['Timestamp']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis #1\n",
    "**Problems:** Can we predict future price of a spot instance given previous history and how other vm’s are reacting?\n",
    "\n",
    "To achieve the goal of prediction, we are expecting to do pattern matching from the collected dataset. In this case, whenever users make a bid, we can based on the resources types, time or day, and the trending price to do pattern matching. We will be able to provide a prediction if we can shoot a pattern.\n",
    "\n",
    "Expecting tools are supervised and unsupervised learning algorithms, e.g. classification and\n",
    "clustering methods.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis #2\n",
    "\n",
    "For each machine type there exists a region that is more favorable to use, as the market volatility is very low and the prices tend to stay cheaper than the other regions.\n",
    "\n",
    "With in proving this hypothesis users will be able to find the best region they should be bidding in, as long as latency is not an issue for them.\n",
    "\n",
    "Data Science tools & Techniques: We can use clustering and classification methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (df.index.min())\n",
    "print (df.index.max())\n",
    "print(df.index.max()- df.index.min()) \n",
    "#df = df.truncate(before='2015-01-16', after='2015-3-18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.resample('H')\n",
    "df = df.fillna(\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create full time series and fill data\n",
    "dfSorted = df.groupby(['AvailabilityZone', 'InstanceType'])\n",
    "dfSorted = dfSorted.resample('H')\n",
    "dfSorted = dfSorted.fillna(\"ffill\")\n",
    "\n",
    "dfSorted=dfSorted.drop('InstanceType', axis=1).drop('AvailabilityZone', axis=1)\n",
    "\n",
    "dfSorted.to_csv(\"im.csv\")\n",
    "depa = pd.read_csv(\"im.csv\")\n",
    "depa = depa.groupby(['AvailabilityZone', 'InstanceType'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#grouped_prices = [group['SpotPrice'].tolist() for name, group in depa]\n",
    "\n",
    "#dfer = dataframe\n",
    "d = {}\n",
    "\n",
    "count = 0\n",
    "#need to run through and get rid of rows where timestamp, spotprice data doesnt exist for the others\n",
    "for name, group in depa:\n",
    "    if count == 0:\n",
    "        d['TimeStamp']=group['TimeStamp'].tolist()\n",
    "    if name[0] ==\"ap-northeast-1a\":\n",
    "        for a in d['TimeStamp']+group['TimeStamp'].tolist():\n",
    "            if(a not in d['TimeStamp']):\n",
    "                group = group[group['TimeStamp'] != a]\n",
    "                print (a)\n",
    "            if(a not in group['TimeStamp'].tolist()):\n",
    "                d['TimeStamp'].remove(a)\n",
    "                print (a)\n",
    "    \n",
    "    \n",
    "    #seter = set(d['TimeStamp']) - set(group['TimeStamp'].tolist())\n",
    "    #print(seter)\n",
    "    #remove = list(seter)\n",
    "    #print(remove)    \n",
    "#dfer = pd.DataFrame(d)\n",
    "    \n",
    "#for name, group in depa:\n",
    "     #print(len(group['SpotPrice'].tolist()))\n",
    "        \n",
    "'''\n",
    "for name, group in depa:\n",
    "    if name[0] ==\"ap-northeast-1a\":\n",
    "        #group.index = group['TimeStamp']\n",
    "        #print(group.head(20))\n",
    "        #group = group.truncate(before='2015-01-18', after='2015-3-17')\n",
    "        d[name[1]]=group['SpotPrice'].tolist()\n",
    "        print(len(group['SpotPrice'].tolist()))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#numpy.corrcoef(grouped_prices)\n",
    "\n",
    "grouped_prices.corr()\n",
    "'''\n",
    "mask = np.zeros_like(corr_df)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "seaborn.heatmap(corr_df, cmap='RdYlGn_r', vmax=1.0, vmin=-1.0 , mask = mask, linewidths=2.5)\n",
    "# Show the plot we reorient the labels for each column and row to make them easier to read.\n",
    "plt.yticks(rotation=0) \n",
    "plt.xticks(rotation=90) \n",
    "plt.show()\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Hypothesis #3\n",
    "\n",
    "There exists some kind of relation between what kind of virtual machines are turning into hotspots. Say that we establish a line as half price of EC2 instances, it makes sense to pay half price to gain usage of resources but probably not more than 3⁄4. By extracting patterns from the price history, we can study that whether or not there was the case that some resources were becoming hotspot in the spot instances market.\n",
    "\n",
    "Potential data science method for this one includes: Time Series, Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('us-east-1a_c3-8xlarge.csv')\n",
    "#df.head(400)\n",
    "df = df2\n",
    "df.describe()\n",
    "\n",
    "df.SpotPrice.plot(label='Spot Price of c3.8xlarge', figsize = (15,5))\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset pre-process\n",
    "\n",
    "Some preprocess work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SpotPrice</th>\n",
       "      <th>InstanceType</th>\n",
       "      <th>OS type</th>\n",
       "      <th>AvailabilityZone</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TimeStamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-02-16 00:54:00</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>m1.small</td>\n",
       "      <td>Linux/UNIX</td>\n",
       "      <td>us-west-1a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-16 00:55:30</th>\n",
       "      <td>2.7320</td>\n",
       "      <td>c3.8xlarge</td>\n",
       "      <td>Linux/UNIX</td>\n",
       "      <td>us-west-1a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-16 00:58:09</th>\n",
       "      <td>0.0679</td>\n",
       "      <td>m3.large</td>\n",
       "      <td>Linux/UNIX</td>\n",
       "      <td>us-west-1b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-16 00:59:56</th>\n",
       "      <td>3.0240</td>\n",
       "      <td>c3.8xlarge</td>\n",
       "      <td>Linux/UNIX</td>\n",
       "      <td>ap-southeast-2a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-16 01:17:30</th>\n",
       "      <td>0.0224</td>\n",
       "      <td>m3.medium</td>\n",
       "      <td>Linux/UNIX</td>\n",
       "      <td>us-east-1d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-16 01:17:37</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>m1.small</td>\n",
       "      <td>Linux/UNIX</td>\n",
       "      <td>us-west-1b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-16 01:24:19</th>\n",
       "      <td>0.7561</td>\n",
       "      <td>c3.2xlarge</td>\n",
       "      <td>Linux/UNIX</td>\n",
       "      <td>ap-southeast-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-16 01:24:19</th>\n",
       "      <td>0.7561</td>\n",
       "      <td>c3.2xlarge</td>\n",
       "      <td>Linux/UNIX</td>\n",
       "      <td>ap-southeast-2a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-16 01:24:25</th>\n",
       "      <td>0.0926</td>\n",
       "      <td>m3.xlarge</td>\n",
       "      <td>Linux/UNIX</td>\n",
       "      <td>ap-southeast-2b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-02-16 01:24:26</th>\n",
       "      <td>3.0241</td>\n",
       "      <td>c3.8xlarge</td>\n",
       "      <td>Linux/UNIX</td>\n",
       "      <td>ap-southeast-2b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     SpotPrice InstanceType     OS type AvailabilityZone\n",
       "TimeStamp                                                               \n",
       "2014-02-16 00:54:00     0.0100     m1.small  Linux/UNIX       us-west-1a\n",
       "2014-02-16 00:55:30     2.7320   c3.8xlarge  Linux/UNIX       us-west-1a\n",
       "2014-02-16 00:58:09     0.0679     m3.large  Linux/UNIX       us-west-1b\n",
       "2014-02-16 00:59:56     3.0240   c3.8xlarge  Linux/UNIX  ap-southeast-2a\n",
       "2014-02-16 01:17:30     0.0224    m3.medium  Linux/UNIX       us-east-1d\n",
       "2014-02-16 01:17:37     0.0100     m1.small  Linux/UNIX       us-west-1b\n",
       "2014-02-16 01:24:19     0.7561   c3.2xlarge  Linux/UNIX  ap-southeast-2b\n",
       "2014-02-16 01:24:19     0.7561   c3.2xlarge  Linux/UNIX  ap-southeast-2a\n",
       "2014-02-16 01:24:25     0.0926    m3.xlarge  Linux/UNIX  ap-southeast-2b\n",
       "2014-02-16 01:24:26     3.0241   c3.8xlarge  Linux/UNIX  ap-southeast-2b"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# parse the data file and extra the results\n",
    "filename = 'aws-spot-price-history/data-1397804701'\n",
    "\n",
    "df = pd.read_csv(filename, sep=\"\\t\", header = None)\n",
    "df.columns = [\"info\", \"SpotPrice\", \"TimeStamp\", \"InstanceType\", \"OS type\", \"AvailabilityZone\"]\n",
    "df['TimeStamp'] =pd.to_datetime(df.TimeStamp)\n",
    "\n",
    "df.index = df.TimeStamp\n",
    "df = df.drop(['TimeStamp'],axis=1).drop('info', 1).sort_index()\n",
    "\n",
    "\n",
    "df.head(10)\n",
    "#df['AvailabilityZone']\n",
    "#df['InstanceType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     SpotPrice InstanceType     OS type AvailabilityZone\n",
      "TimeStamp                                                               \n",
      "2014-02-16 21:37:34        2.0     c3.large  Linux/UNIX       us-west-1a\n",
      "2014-02-17 21:38:26        2.0     c3.large  Linux/UNIX       us-west-1a\n",
      "2014-02-18 21:38:35        2.0     c3.large  Linux/UNIX       us-west-1a\n",
      "2014-02-19 21:39:40        2.0     c3.large  Linux/UNIX       us-west-1a\n",
      "2014-02-20 21:40:04        2.0     c3.large  Linux/UNIX       us-west-1a\n",
      "                     SpotPrice InstanceType     OS type AvailabilityZone\n",
      "TimeStamp                                                               \n",
      "2014-02-16 18:13:46     0.3421    c3.xlarge  Linux/UNIX       us-west-1a\n",
      "2014-02-17 18:13:48     0.3421    c3.xlarge  Linux/UNIX       us-west-1a\n",
      "2014-02-18 18:14:32     0.3421    c3.xlarge  Linux/UNIX       us-west-1a\n",
      "2014-02-19 18:15:16     0.3421    c3.xlarge  Linux/UNIX       us-west-1a\n",
      "2014-02-20 18:15:20     0.3421    c3.xlarge  Linux/UNIX       us-west-1a\n",
      "                     SpotPrice InstanceType     OS type AvailabilityZone\n",
      "TimeStamp                                                               \n",
      "2014-02-16 05:33:25     0.6831   c3.2xlarge  Linux/UNIX       us-west-1a\n",
      "2014-02-17 05:33:44     0.6831   c3.2xlarge  Linux/UNIX       us-west-1a\n",
      "2014-02-17 17:50:20     1.0000   c3.2xlarge  Linux/UNIX       us-west-1a\n",
      "2014-02-17 18:59:52     0.6831   c3.2xlarge  Linux/UNIX       us-west-1a\n",
      "2014-02-18 19:00:50     0.6831   c3.2xlarge  Linux/UNIX       us-west-1a\n",
      "                     SpotPrice InstanceType     OS type AvailabilityZone\n",
      "TimeStamp                                                               \n",
      "2014-02-16 10:58:26       0.40   c3.4xlarge  Linux/UNIX       us-west-1a\n",
      "2014-02-17 10:58:56       0.40   c3.4xlarge  Linux/UNIX       us-west-1a\n",
      "2014-02-18 00:32:45       0.45   c3.4xlarge  Linux/UNIX       us-west-1a\n",
      "2014-02-19 00:33:08       0.45   c3.4xlarge  Linux/UNIX       us-west-1a\n",
      "2014-02-19 11:49:14       0.75   c3.4xlarge  Linux/UNIX       us-west-1a\n",
      "                     SpotPrice InstanceType     OS type AvailabilityZone\n",
      "TimeStamp                                                               \n",
      "2014-02-16 00:55:30      2.732   c3.8xlarge  Linux/UNIX       us-west-1a\n",
      "2014-02-17 00:56:30      2.732   c3.8xlarge  Linux/UNIX       us-west-1a\n",
      "2014-02-18 00:57:12      2.732   c3.8xlarge  Linux/UNIX       us-west-1a\n",
      "2014-02-19 00:57:56      2.732   c3.8xlarge  Linux/UNIX       us-west-1a\n",
      "2014-02-20 00:58:42      2.732   c3.8xlarge  Linux/UNIX       us-west-1a\n",
      "                     SpotPrice InstanceType     OS type AvailabilityZone\n",
      "TimeStamp                                                               \n",
      "2014-02-16 18:58:27     0.1711     c3.large  Linux/UNIX       us-west-1b\n",
      "2014-02-17 18:58:36     0.1711     c3.large  Linux/UNIX       us-west-1b\n",
      "2014-02-18 18:59:34     0.1711     c3.large  Linux/UNIX       us-west-1b\n",
      "2014-02-19 19:00:43     0.1711     c3.large  Linux/UNIX       us-west-1b\n",
      "2014-02-20 19:01:44     0.1711     c3.large  Linux/UNIX       us-west-1b\n",
      "                     SpotPrice InstanceType     OS type AvailabilityZone\n",
      "TimeStamp                                                               \n",
      "2014-02-16 05:50:43        2.0    c3.xlarge  Linux/UNIX       us-west-1b\n",
      "2014-02-17 05:51:04        2.0    c3.xlarge  Linux/UNIX       us-west-1b\n",
      "2014-02-18 05:51:04        2.0    c3.xlarge  Linux/UNIX       us-west-1b\n",
      "2014-02-19 05:51:21        2.0    c3.xlarge  Linux/UNIX       us-west-1b\n",
      "2014-02-20 05:51:56        2.0    c3.xlarge  Linux/UNIX       us-west-1b\n",
      "                     SpotPrice InstanceType     OS type AvailabilityZone\n",
      "TimeStamp                                                               \n",
      "2014-02-16 18:58:22     0.6831   c3.2xlarge  Linux/UNIX       us-west-1b\n",
      "2014-02-17 18:58:30     0.6831   c3.2xlarge  Linux/UNIX       us-west-1b\n",
      "2014-02-18 00:43:27     0.7500   c3.2xlarge  Linux/UNIX       us-west-1b\n",
      "2014-02-19 00:43:59     0.7500   c3.2xlarge  Linux/UNIX       us-west-1b\n",
      "2014-02-19 10:47:14     0.8000   c3.2xlarge  Linux/UNIX       us-west-1b\n",
      "                     SpotPrice InstanceType     OS type AvailabilityZone\n",
      "TimeStamp                                                               \n",
      "2014-02-16 21:24:27        2.0   c3.4xlarge  Linux/UNIX       us-west-1b\n",
      "2014-02-17 21:24:57        2.0   c3.4xlarge  Linux/UNIX       us-west-1b\n",
      "2014-02-18 21:25:03        2.0   c3.4xlarge  Linux/UNIX       us-west-1b\n",
      "2014-02-19 21:26:00        2.0   c3.4xlarge  Linux/UNIX       us-west-1b\n",
      "2014-02-20 21:26:23        2.0   c3.4xlarge  Linux/UNIX       us-west-1b\n",
      "                     SpotPrice InstanceType     OS type AvailabilityZone\n",
      "TimeStamp                                                               \n",
      "2014-02-16 05:52:00      2.732   c3.8xlarge  Linux/UNIX       us-west-1b\n",
      "2014-02-17 05:52:21      2.732   c3.8xlarge  Linux/UNIX       us-west-1b\n",
      "2014-02-18 05:52:24      2.732   c3.8xlarge  Linux/UNIX       us-west-1b\n",
      "2014-02-19 05:52:42      2.732   c3.8xlarge  Linux/UNIX       us-west-1b\n",
      "2014-02-20 05:53:17      2.732   c3.8xlarge  Linux/UNIX       us-west-1b\n"
     ]
    }
   ],
   "source": [
    "region_list = [\"us-west-1a\",\"us-west-1b\" ]\n",
    "vm_types_list = [\"c3.large\", \"c3.xlarge\", \"c3.2xlarge\",\"c3.4xlarge\", \"c3.8xlarge\"]\n",
    "\n",
    "dicts = {}\n",
    "\n",
    "for r in region_list:\n",
    "    r = {}\n",
    "    for t in vm_types_list:\n",
    "        #print(t)\n",
    "        df1 = df[df.AvailabilityZone == r]\n",
    "        df2 = df1[df1.InstanceType == t]\n",
    "        #print(df2.head(5))\n",
    "        # get the time series data\n",
    "        r[t] = df2[\"SpotPrice\"].tolist()\n",
    "\n",
    "print(len())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "us-west-1a\n",
      "c3.large\n",
      "c3.xlarge\n",
      "c3.2xlarge\n",
      "c3.4xlarge\n",
      "c3.8xlarge\n",
      "us-west-1b\n",
      "c3.large\n",
      "c3.xlarge\n",
      "c3.2xlarge\n",
      "c3.4xlarge\n",
      "c3.8xlarge\n",
      "2\n",
      "TimeStamp\n",
      "2014-02-16 00:55:30    2.7320\n",
      "2014-02-17 00:56:30    2.7320\n",
      "2014-02-18 00:57:12    2.7320\n",
      "2014-02-19 00:57:56    2.7320\n",
      "2014-02-20 00:58:42    2.7320\n",
      "2014-02-21 00:58:59    2.7320\n",
      "2014-02-21 07:59:55    3.0000\n",
      "2014-02-21 08:59:47    2.7321\n",
      "2014-02-22 09:00:21    2.7321\n",
      "2014-02-23 09:01:02    2.7321\n",
      "2014-02-24 09:01:29    2.7321\n",
      "2014-02-25 09:02:15    2.7321\n",
      "2014-02-25 19:00:49    0.4991\n",
      "2014-02-25 19:02:11    0.4073\n",
      "2014-02-25 20:31:27    0.4088\n",
      "2014-02-25 22:02:16    0.4077\n",
      "2014-02-25 22:56:06    0.4087\n",
      "2014-02-26 00:02:24    0.4077\n",
      "2014-02-26 00:32:48    0.4087\n",
      "2014-02-26 01:43:26    0.4076\n",
      "2014-02-26 05:10:18    0.4100\n",
      "2014-02-26 06:19:01    0.4089\n",
      "2014-02-26 08:04:32    0.4155\n",
      "2014-02-26 08:42:55    0.4277\n",
      "2014-02-26 09:06:20    0.4295\n",
      "2014-02-26 09:09:05    0.4340\n",
      "2014-02-26 09:11:51    0.4316\n",
      "2014-02-26 09:14:37    0.4295\n",
      "2014-02-26 09:15:59    0.4340\n",
      "2014-02-26 09:39:26    0.4367\n",
      "                        ...  \n",
      "2014-04-07 10:55:38    2.0000\n",
      "2014-04-07 10:59:04    2.7321\n",
      "2014-04-07 13:56:28    2.0000\n",
      "2014-04-07 13:59:50    2.7321\n",
      "2014-04-07 15:25:06    2.0000\n",
      "2014-04-07 15:28:26    2.7321\n",
      "2014-04-07 16:28:29    2.0000\n",
      "2014-04-07 16:31:50    2.7321\n",
      "2014-04-07 17:01:59    2.0000\n",
      "2014-04-07 17:05:16    2.7321\n",
      "2014-04-07 18:02:10    2.0000\n",
      "2014-04-07 18:05:27    2.7321\n",
      "2014-04-07 20:15:24    9.0000\n",
      "2014-04-08 05:03:21    9.6000\n",
      "2014-04-09 05:05:33    9.6000\n",
      "2014-04-09 22:52:30    9.0000\n",
      "2014-04-10 04:35:20    9.6000\n",
      "2014-04-11 04:35:52    9.6000\n",
      "2014-04-11 19:36:51    2.7321\n",
      "2014-04-11 19:40:11    2.0000\n",
      "2014-04-11 21:07:00    9.6000\n",
      "2014-04-12 21:08:18    9.6000\n",
      "2014-04-13 21:10:04    9.6000\n",
      "2014-04-14 21:11:17    9.6000\n",
      "2014-04-15 21:12:26    9.6000\n",
      "2014-04-16 03:33:35    9.0000\n",
      "2014-04-16 05:08:37    9.6000\n",
      "2014-04-17 01:49:43    9.0000\n",
      "2014-04-17 01:51:42    2.0000\n",
      "2014-04-17 08:05:28    9.6000\n",
      "Name: SpotPrice, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ninstance_types  = [\\'c3.xlarge\\', \\'c3.2xlarge\\', \\'c3.4xlarge\\', \\'c3.8xlarge\\']\\nregion1 = \\'us-east-1\\'\\n\\ndf1 = df[df.AvailabilityZone == \\'us-west-1a\\']\\ndf2 = df1[df1.InstanceType == \\'c3.8xlarge\\']\\ndf2.to_csv(\\'us-east-1a_c3-8xlarge.csv\\')\\ndflist_8x = df2[\"SpotPrice\"]\\n\\n\\ndf2 = df1[df1.InstanceType == \\'c3.4xlarge\\']\\ndf2.to_csv(\\'us-east-1a_c3-4xlarge.csv\\')\\ndflist_4x = df2[\"SpotPrice\"]\\n\\n\\ndf2 = df1[df1.InstanceType == \\'c3.2xlarge\\']\\ndf2.to_csv(\\'us-east-1a_c3-2xlarge.csv\\')\\ndflist_2x = df2[\"SpotPrice\"]\\n\\n\\ndf2 = df1[df1.InstanceType == \\'c3.xlarge\\']\\ndf2.to_csv(\\'us-east-1a_c3-xlarge.csv\\')\\ndflist_x = df2[\"SpotPrice\"]\\n\\n\\n\\nfor i in instance_types:\\n    print(i)\\n    df1 = df[df.AvailabilityZone == region1]\\n    df2 = df1[df1.InstanceType == i]\\n    #df2.to_csv(i)\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "different_regions = [\n",
    "\n",
    "    df_us_west_one_a = df[df.AvailabilityZone == \"us-west-1a\"],\n",
    "    df_us_west_one_b = df[df.AvailabilityZone == \"us-west-1b\"],\n",
    "\n",
    "    df_us_east_one_a = df[df.AvailabilityZone == \"us-east-1a\"],\n",
    "    df_us_east_one_b = df[df.AvailabilityZone == \"us-east-1b\"],\n",
    "    df_us_east_one_c = df[df.AvailabilityZone == \"us-east-1c\"],\n",
    "    df_us_east_one_d = df[df.AvailabilityZone == \"us-east-1d\"],\n",
    "\n",
    "    df_ap_southeast_one_a = df[df.AvailabilityZone == \"ap-southeast-1a\"],\n",
    "    df_ap_southeast_one_b = df[df.AvailabilityZone == \"ap-southeast-1b\"],\n",
    "\n",
    "    df_ap_southeast_two_a = df[df.AvailabilityZone == \"ap-southeast-2a\"],\n",
    "    df_ap_southeast_two_b = df[df.AvailabilityZone == \"ap-southeast-2b\"]\n",
    "\n",
    "    df[df.AvailabilityZone == \"us-west-1a\"],\n",
    "    df[df.AvailabilityZone == \"us-west-1b\"],\n",
    "\n",
    "    df[df.AvailabilityZone == \"us-east-1a\"],\n",
    "    df[df.AvailabilityZone == \"us-east-1b\"],\n",
    "    df[df.AvailabilityZone == \"us-east-1c\"],\n",
    "    df[df.AvailabilityZone == \"us-east-1d\"],\n",
    "\n",
    "    df[df.AvailabilityZone == \"ap-southeast-1a\"],\n",
    "    df[df.AvailabilityZone == \"ap-southeast-1b\"],\n",
    "\n",
    "    df[df.AvailabilityZone == \"ap-southeast-2a\"],\n",
    "    df[df.AvailabilityZone == \"ap-southeast-2b\"]\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "df_us_west_one_a = df[df.AvailabilityZone == \"us-west-1a\"]\n",
    "df_us_west_one_b = df[df.AvailabilityZone == \"us-west-1b\"]\n",
    "\n",
    "df_us_east_one_a = df[df.AvailabilityZone == \"us-east-1a\"]\n",
    "df_us_east_one_b = df[df.AvailabilityZone == \"us-east-1b\"]\n",
    "df_us_east_one_c = df[df.AvailabilityZone == \"us-east-1c\"]\n",
    "df_us_east_one_d = df[df.AvailabilityZone == \"us-east-1d\"]\n",
    "\n",
    "df_ap_southeast_one_a = df[df.AvailabilityZone == \"ap-southeast-1a\"]\n",
    "df_ap_southeast_one_b = df[df.AvailabilityZone == \"ap-southeast-1b\"]\n",
    "\n",
    "df_ap_southeast_two_a = df[df.AvailabilityZone == \"ap-southeast-2a\"]\n",
    "df_ap_southeast_two_b = df[df.AvailabilityZone == \"ap-southeast-2b\"]\n",
    "\n",
    "#c3,c3_x,c3_2x,c3_4x,c3_8x = [],[],[],[],[]\n",
    "#c3_8x = {}\n",
    "#for dataframe in df_us_west_one_a,df_us_west_one_b,df_ap_southeast_two_a,df_ap_southeast_two_b, \\\n",
    "    #df_us_east_one_a,df_us_east_one_b,df_us_east_one_c,df_us_east_one_d,\\\n",
    "    #df_ap_southeast_one_a, df_ap_southeast_one_b:\n",
    "    \n",
    "    #df2 = dataframe[dataframe.InstanceType == 'c3.8xlarge']\n",
    "    #dflist = df2[\"SpotPrice\"]\n",
    "    #c3_8x.append(dflist)\n",
    "    #c3_8x[\"1\"] = dflist\n",
    "#print(c3_8x)\n",
    "\n",
    "\n",
    "c3_8x = {}\n",
    "for dff in df_us_west_one_a,df_us_west_one_b,df_ap_southeast_two_a,df_ap_southeast_two_b, \\\n",
    "    df_us_east_one_a,df_us_east_one_b,df_us_east_one_c,df_us_east_one_d,\\\n",
    "    df_ap_southeast_one_a, df_ap_southeast_one_b:\n",
    "    \n",
    "    df2 = df[df.InstanceType == 'c3.8xlarge']\n",
    "    dflist = df2[\"SpotPrice\"]\n",
    "    \n",
    "    \n",
    "    dataframe = df_us_west_one_a\n",
    "df2 = dataframe[dataframe.InstanceType == 'c3.8xlarge']\n",
    "dflist = df2[\"SpotPrice\"]\n",
    "c3_8x[\"us_west_one_a\"] = dflist\n",
    "\n",
    "dataframe = df_us_west_one_b\n",
    "df2 = dataframe[dataframe.InstanceType == 'c3.8xlarge']\n",
    "dflist = df2[\"SpotPrice\"]\n",
    "c3_8x[\"us_west_one_b\"] = dflist\n",
    "\n",
    "print(len(c3_8x))\n",
    "print(c3_8x[\"us_west_one_a\"])\n",
    "\n",
    "\n",
    "#variations = [dflist_x.tolist()[:1000], dflist_2x.tolist()[:1000], dflist_4x.tolist()[:1000], dflist_8x.tolist()[:1000]]\n",
    "\"\"\"\n",
    "instance_types  = ['c3.xlarge', 'c3.2xlarge', 'c3.4xlarge', 'c3.8xlarge']\n",
    "region1 = 'us-east-1'\n",
    "\n",
    "df1 = df[df.AvailabilityZone == 'us-west-1a']\n",
    "df2 = df1[df1.InstanceType == 'c3.8xlarge']\n",
    "df2.to_csv('us-east-1a_c3-8xlarge.csv')\n",
    "dflist_8x = df2[\"SpotPrice\"]\n",
    "\n",
    "\n",
    "df2 = df1[df1.InstanceType == 'c3.4xlarge']\n",
    "df2.to_csv('us-east-1a_c3-4xlarge.csv')\n",
    "dflist_4x = df2[\"SpotPrice\"]\n",
    "\n",
    "\n",
    "df2 = df1[df1.InstanceType == 'c3.2xlarge']\n",
    "df2.to_csv('us-east-1a_c3-2xlarge.csv')\n",
    "dflist_2x = df2[\"SpotPrice\"]\n",
    "\n",
    "\n",
    "df2 = df1[df1.InstanceType == 'c3.xlarge']\n",
    "df2.to_csv('us-east-1a_c3-xlarge.csv')\n",
    "dflist_x = df2[\"SpotPrice\"]\n",
    "\n",
    "\n",
    "\n",
    "for i in instance_types:\n",
    "    print(i)\n",
    "    df1 = df[df.AvailabilityZone == region1]\n",
    "    df2 = df1[df1.InstanceType == i]\n",
    "    #df2.to_csv(i)\n",
    "\"\"\"\n",
    "#df2.head(20)\n",
    "#df2[\"SpotPrice\"]\n",
    "#df_ap_southeast_two_b.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for k, g in df1.sort_index(ascending=True).groupby(['InstanceType'], as_index=False):\n",
    "    for key, grp in g.groupby(['AvailabilityZone'], as_index=False):\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.plot(grp.index, grp['SpotPrice'], label=key)\n",
    "        \n",
    "    plt.legend()\n",
    "    plt.title('Spot Pricing - ' + k)\n",
    "    plt.show()\n",
    "\n",
    "for k, g in df1.sort_index(ascending=True).groupby(['InstanceType'], as_index=False):\n",
    "    #plt.figure(1, figsize(20,5))\n",
    "    for key, grp in g.groupby(['AvailabilityZone'], as_index=False):\n",
    "        plt.figure(figsize=(15,5))\n",
    "        plt.hist(grp['SpotPrice'], bins=100, label=key,)\n",
    "        plt.xlim([0, 1])\n",
    "        #grp.groupby(grp.index.dayofweek).agg(['mean']).plot()\n",
    "    plt.legend()\n",
    "    plt.title('Histogram of Spot Pricing - ' + k)\n",
    "    plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Clustering\n",
    "\n",
    "## Time series clustering on spot market price data set.\n",
    "\n",
    "Here we will be using TSC to analyse relations between various of types of machines and find out relationships from our clustering results.\n",
    "\n",
    "The first step is to work out an appropriate distance/similarity metric. Secondly, we will use existing clustering techniques, such as k-means, hierarchical clustering, density-based clustering or subspace clustering, to find clustering structures.\n",
    "\n",
    "Dynamic Time Warping (DTW) finds optimal alignment between two time series, and DTW distance is used as a distance metric in the example below.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "A data set of Synthetic Control Chart Time Series is used here, which contains 600 examples of control charts. Each control chart is a time series with 60 values. There are six classes: 1) 1-100 Normal, 2) 101-200 Cyclic, 3) 201-300 Increasing trend, 4)301-400 Decreasing trend, 5) 401-500 Upward shift, and 6) 501-600 Downward shift. The dataset is downloadable at UCI KDD Archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(__doc__)\n",
    "\n",
    "# Author: Gael Varoquaux gael.varoquaux@normalesup.org\n",
    "# License: BSD 3 clause\n",
    "\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    from matplotlib.finance import quotes_historical_yahoo_ochl\n",
    "except ImportError:\n",
    "    # quotes_historical_yahoo_ochl was named quotes_historical_yahoo before matplotlib 1.4\n",
    "    from matplotlib.finance import quotes_historical_yahoo as quotes_historical_yahoo_ochl\n",
    "from matplotlib.collections import LineCollection\n",
    "from sklearn import cluster, covariance, manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Choose a time period reasonably calm (not too long ago so that we get\n",
    "# high-tech firms, and before the 2008 crash)\n",
    "\"\"\"\n",
    "\n",
    "d1 = datetime.datetime(2003, 1, 1)\n",
    "d2 = datetime.datetime(2008, 1, 1)\n",
    "\n",
    "symbols, names = np.array(list(symbol_dict.items())).T\n",
    "#print(symbols, names)\n",
    "\n",
    "quotes = [quotes_historical_yahoo_ochl(symbol, d1, d2, asobject=True)\n",
    "          for symbol in symbols]\n",
    "\n",
    "open = np.array([q.open for q in quotes]).astype(np.float)\n",
    "close = np.array([q.close for q in quotes]).astype(np.float)\n",
    "\n",
    "# The daily variations of the quotes are what carry most information\n",
    "variation = close - open\n",
    "\n",
    "#print(variation)\n",
    "print(len(variation))\n",
    "type(variation)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.3421  0.3421  0.3421 ...,  0.0519  0.052   0.0521]\n",
      " [ 0.6831  0.6831  1.     ...,  0.1519  0.1384  0.1238]\n",
      " [ 0.4     0.4     0.45   ...,  0.1287  0.1288  0.129 ]\n",
      " [ 2.732   2.732   2.732  ...,  0.4064  0.4063  0.4064]]\n"
     ]
    }
   ],
   "source": [
    "#list_x = dflist_x.tolist()\n",
    "\n",
    "variations = [dflist_x.tolist()[:1000], dflist_2x.tolist()[:1000], dflist_4x.tolist()[:1000], dflist_8x.tolist()[:1000]]\n",
    "#print(variations)\n",
    "#prices = np.array([q.open for q in quotes]).astype(np.float)\n",
    "#print(len(variations[0]))\n",
    "prices = np.array(variations)\n",
    "\n",
    "print(prices)\n",
    "#prices.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GraphLassoCV(alphas=4, assume_centered=False, cv=None, enet_tol=0.0001,\n",
       "       max_iter=100, mode='cd', n_jobs=1, n_refinements=4, tol=0.0001,\n",
       "       verbose=False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_model = covariance.GraphLassoCV()\n",
    "\n",
    "# standardize the time series: using correlations rather than covariance\n",
    "# is more efficient for structure recovery\n",
    "#X = variation.copy().T\n",
    "X = prices.copy().T\n",
    "print(type(X))\n",
    "#print(X)\n",
    "\n",
    "#X /= X.std(axis=0)\n",
    "X = X / X.std(axis=0)\n",
    "\n",
    "edge_model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-52c648afce5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_labels\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cluster %i: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'names' is not defined"
     ]
    }
   ],
   "source": [
    "_, labels = cluster.affinity_propagation(edge_model.covariance_)\n",
    "n_labels = labels.max()\n",
    "\n",
    "for i in range(n_labels + 1):\n",
    "    print('Cluster %i: %s' % ((i + 1), ', '.join(names[labels == i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We use a dense eigen_solver to achieve reproducibility (arpack is\n",
    "# initiated with random vectors that we don't control). In addition, we\n",
    "# use a large number of neighbors to capture the large-scale structure.\n",
    "node_position_model = manifold.LocallyLinearEmbedding(\n",
    "    n_components=2, eigen_solver='dense', n_neighbors=1)\n",
    "\n",
    "embedding = node_position_model.fit_transform(X.T).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1, facecolor='w', figsize=(10, 8))\n",
    "plt.clf()\n",
    "ax = plt.axes([0., 0., 1., 1.])\n",
    "plt.axis('off')\n",
    "\n",
    "# Display a graph of the partial correlations\n",
    "partial_correlations = edge_model.precision_.copy()\n",
    "d = 1 / np.sqrt(np.diag(partial_correlations))\n",
    "partial_correlations *= d\n",
    "partial_correlations *= d[:, np.newaxis]\n",
    "non_zero = (np.abs(np.triu(partial_correlations, k=1)) > 0.02)\n",
    "\n",
    "# Plot the nodes using the coordinates of our embedding\n",
    "plt.scatter(embedding[0], embedding[1], s=100 * d ** 2, c=labels,\n",
    "            cmap=plt.cm.spectral)\n",
    "\n",
    "# Plot the edges\n",
    "start_idx, end_idx = np.where(non_zero)\n",
    "#a sequence of (*line0*, *line1*, *line2*), where::\n",
    "#            linen = (x0, y0), (x1, y1), ... (xm, ym)\n",
    "segments = [[embedding[:, start], embedding[:, stop]]\n",
    "            for start, stop in zip(start_idx, end_idx)]\n",
    "values = np.abs(partial_correlations[non_zero])\n",
    "lc = LineCollection(segments,\n",
    "                    zorder=0, cmap=plt.cm.hot_r,\n",
    "                    norm=plt.Normalize(0, .7 * values.max()))\n",
    "lc.set_array(values)\n",
    "lc.set_linewidths(15 * values)\n",
    "ax.add_collection(lc)\n",
    "\n",
    "# Add a label to each node. The challenge here is that we want to\n",
    "# position the labels to avoid overlap with other labels\n",
    "for index, (name, label, (x, y)) in enumerate(\n",
    "        zip(names, labels, embedding.T)):\n",
    "\n",
    "    dx = x - embedding[0]\n",
    "    dx[index] = 1\n",
    "    dy = y - embedding[1]\n",
    "    dy[index] = 1\n",
    "    this_dx = dx[np.argmin(np.abs(dy))]\n",
    "    this_dy = dy[np.argmin(np.abs(dx))]\n",
    "    if this_dx > 0:\n",
    "        horizontalalignment = 'left'\n",
    "        x = x + .002\n",
    "    else:\n",
    "        horizontalalignment = 'right'\n",
    "        x = x - .002\n",
    "    if this_dy > 0:\n",
    "        verticalalignment = 'bottom'\n",
    "        y = y + .002\n",
    "    else:\n",
    "        verticalalignment = 'top'\n",
    "        y = y - .002\n",
    "    plt.text(x, y, name, size=10,\n",
    "             horizontalalignment=horizontalalignment,\n",
    "             verticalalignment=verticalalignment,\n",
    "             bbox=dict(facecolor='w',\n",
    "                       edgecolor=plt.cm.spectral(label / float(n_labels)),\n",
    "                       alpha=.6))\n",
    "\n",
    "plt.xlim(embedding[0].min() - .15 * embedding[0].ptp(),\n",
    "         embedding[0].max() + .10 * embedding[0].ptp(),)\n",
    "plt.ylim(embedding[1].min() - .03 * embedding[1].ptp(),\n",
    "         embedding[1].max() + .03 * embedding[1].ptp())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TS Clustering Try #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import seaborn as sns;\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import scipy.cluster.hierarchy as hac\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_samples = 10\n",
    "group_size = 4\n",
    "\n",
    "x = np.linspace(0, 15, num_samples)\n",
    "a = np.sin(x) + np.linspace(0, 5, num_samples)\n",
    "\n",
    "x = np.linspace(0, 50, num_samples)\n",
    "b = np.sin(x) + np.linspace(0, -8, num_samples)\n",
    "c = np.sin(x + 2)\n",
    "\n",
    "d = np.linspace(0, 14, num_samples)\n",
    "e = np.random.randn(group_size, 1) + np.linspace(0, -3, num_samples)\n",
    "\n",
    "x = np.linspace(0, 4, num_samples)\n",
    "f = np.sin(x)\n",
    "\n",
    "variances = [dflist_x.tolist()[:1000], dflist_2x.tolist()[:1000], dflist_4x.tolist()[:1000], dflist_8x.tolist()[:1000]]\n",
    "\n",
    "\n",
    "timeSeries = pd.DataFrame()\n",
    "ax = None\n",
    "\n",
    "#for arr in [a,b,c,d,e,f]:\n",
    "for arr in [dflist_x.tolist()[:1000], dflist_2x.tolist()[:1000], dflist_4x.tolist()[:1000], dflist_8x.tolist()[:1000]]:    \n",
    "    #print(arr)\n",
    "    print(np.random.rand(group_size, num_samples))\n",
    "    print((np.random.randn(group_size, 1)*3))\n",
    "    #arr = arr + np.random.rand(group_size, num_samples) + (np.random.randn(group_size, 1)*3)\n",
    "    #print(arr)\n",
    "    df = pd.DataFrame(arr)\n",
    "    timeSeries = timeSeries.append(df)\n",
    "\n",
    "    # We use seaborn to plot what we have\n",
    "    #ax = sns.tsplot(ax=ax, data=df.values, ci=[68, 95])\n",
    "    #ax = sns.tsplot(ax=ax, data=df.values, err_style=\"unit_traces\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Just one line :)\n",
    "Z = hac.linkage(timeSeries, 'single', 'correlation')\n",
    "\n",
    "# Plot the dendogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "hac.dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=8.,  # font size for the x axis labels\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Here we decided to use spearman correlation\n",
    "correlation_matrix = timeSeries.T.corr(method='spearman')\n",
    "\n",
    "# Do the clustering\n",
    "Z = hac.linkage(correlation_matrix, 'single')\n",
    "\n",
    "# Plot dendogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('distance')\n",
    "hac.dendrogram(\n",
    "    Z,\n",
    "    leaf_rotation=90.,  # rotates the x axis labels\n",
    "    leaf_font_size=8.,  # font size for the x axis labels\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "1. [](http://scikit-learn.org/stable/modules/generated/sklearn.covariance.GraphLassoCV.html)\n",
    "2. [](http://scikit-learn.org/stable/modules/covariance.html)\n",
    "\n",
    "\n",
    "1. [Mining Time-series with Trillions of Points: Dynamic Time Warping at scale](http://practicalquant.blogspot.com/2012/10/mining-time-series-with-trillions-of.html)\n",
    "2. [sackoverflow post](http://stackoverflow.com/questions/34940808/hierarchical-clustering-of-time-series-in-python-scipy-numpy-pandas)\n",
    "3. [scilearn material](http://scikit-learn.org/stable/modules/clustering.html)\n",
    "4. [pattern matching over time series data](http://stats.stackexchange.com/questions/136091/sequential-pattern-matching-in-time-series-data)\n",
    "\n",
    "http://sklearn.lzjqsdd.com/modules/covariance.html"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
